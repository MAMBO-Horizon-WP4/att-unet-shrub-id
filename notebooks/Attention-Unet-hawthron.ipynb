{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSBAjcUUinVu"
   },
   "source": [
    "# Connect with Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23415,
     "status": "ok",
     "timestamp": 1727678599314,
     "user": {
      "displayName": "Ce Zhang",
      "userId": "07925141029290984259"
     },
     "user_tz": -60
    },
    "id": "ZTOR4iVzigri",
    "outputId": "009241e5-86a0-4da3-f45c-6449c44b214a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/users/wrmod/rafbar/Projs/mambo-dl/Hawthron_DL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-3znE9_lyxV"
   },
   "source": [
    "\n",
    "# Attention UNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynBnk6Syl8k5"
   },
   "source": [
    "## Attention UNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1727369605222,
     "user": {
      "displayName": "dazhuang wang",
      "userId": "16842954630206696630"
     },
     "user_tz": -60
    },
    "id": "yRHuKSbnr4js",
    "outputId": "3feb2b55-e855-45e5-949e-a18cce79ae22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttUNet(\n",
      "  (Maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (Conv1): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Conv2): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Conv3): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Conv4): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Conv5): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Up5): up_conv(\n",
      "    (up): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Att5): Attention_block(\n",
      "    (W_g): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (W_x): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (psi): Sequential(\n",
      "      (0): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Sigmoid()\n",
      "    )\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (Up_conv5): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Up4): up_conv(\n",
      "    (up): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Att4): Attention_block(\n",
      "    (W_g): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (W_x): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (psi): Sequential(\n",
      "      (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Sigmoid()\n",
      "    )\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (Up_conv4): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Up3): up_conv(\n",
      "    (up): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Att3): Attention_block(\n",
      "    (W_g): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (W_x): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (psi): Sequential(\n",
      "      (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Sigmoid()\n",
      "    )\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (Up_conv3): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Up2): up_conv(\n",
      "    (up): Sequential(\n",
      "      (0): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Att2): Attention_block(\n",
      "    (W_g): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (W_x): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (psi): Sequential(\n",
      "      (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Sigmoid()\n",
      "    )\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (Up_conv2): conv_block(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (Conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(conv_block, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Up Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(Attention_block, self).__init__()\n",
    "\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        out = x * psi\n",
    "        return out\n",
    "\n",
    "class AttUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Unet implementation\n",
    "    Paper: https://arxiv.org/abs/1804.03999\n",
    "    \"\"\"\n",
    "    def __init__(self, img_ch=3, output_ch=1):\n",
    "        super(AttUNet, self).__init__()\n",
    "\n",
    "        n1 = 64\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(img_ch, filters[0])\n",
    "        self.Conv2 = conv_block(filters[0], filters[1])\n",
    "        self.Conv3 = conv_block(filters[1], filters[2])\n",
    "        self.Conv4 = conv_block(filters[2], filters[3])\n",
    "        self.Conv5 = conv_block(filters[3], filters[4])\n",
    "\n",
    "        self.Up5 = up_conv(filters[4], filters[3])\n",
    "        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n",
    "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
    "\n",
    "        self.Up4 = up_conv(filters[3], filters[2])\n",
    "        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n",
    "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
    "\n",
    "        self.Up3 = up_conv(filters[2], filters[1])\n",
    "        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n",
    "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
    "\n",
    "        self.Up2 = up_conv(filters[1], filters[0])\n",
    "        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n",
    "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
    "\n",
    "        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.Maxpool1(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.Maxpool2(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.Maxpool3(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.Maxpool4(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        d5 = self.Up5(e5)\n",
    "        x4 = self.Att5(g=d5, x=e4)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4, x=e3)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3, x=e2)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2, x=e1)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Test the Attention UNet network\n",
    "model = AttUNet(img_ch=3, output_ch=1).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hRgDl2BmMYn"
   },
   "source": [
    "## Custom Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40295,
     "status": "ok",
     "timestamp": 1727369646688,
     "user": {
      "displayName": "dazhuang wang",
      "userId": "16842954630206696630"
     },
     "user_tz": -60
    },
    "id": "ypyfde8bmIwq",
    "outputId": "176a81fd-f644-43e6-be38-a0658c000fe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([4, 3, 256, 256])\n",
      "Labels shape: torch.Size([4, 1, 256, 256])\n",
      "Images min: 0.0, max: 1.0\n",
      "Labels min: 0.0, max: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class RSDataset(Dataset):\n",
    "    def __init__(self, images_dir, labels_dir, transform=None, max_samples=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "        self.label_files = sorted(os.listdir(labels_dir))\n",
    "        if max_samples:\n",
    "            self.image_files = self.image_files[:max_samples]\n",
    "            self.label_files = self.label_files[:max_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.labels_dir, self.label_files[idx])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = Image.open(label_path).convert(\"L\")\n",
    "\n",
    "        image = np.array(image)\n",
    "        label = np.array(label)\n",
    "\n",
    "        # Normalize image to [0, 1]\n",
    "        image = image / 255.0\n",
    "\n",
    "        # Normalize label to [0, 1]\n",
    "        label = label / 255.0\n",
    "\n",
    "        # Add channel dimension to label\n",
    "        label = np.expand_dims(label, axis=0)\n",
    "\n",
    "        # Adjust the dimensions of the image to [channels, height, width]\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Example usage:\n",
    "images_dir = 'resized_train_image_no_flip'\n",
    "labels_dir = 'binarized_resized_train_label_no_flip'\n",
    "\n",
    "dataset = RSDataset(images_dir, labels_dir, max_samples=2500)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Get a batch of images and labels\n",
    "images, labels = next(iter(dataloader))\n",
    "\n",
    "# Print the shape of images and labels\n",
    "print(f'Images shape: {images.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "\n",
    "# Check if the images are in the expected format (e.g., [batch_size, channels, height, width])\n",
    "expected_channels = 3  # Adjust this if your images have a different number of channels\n",
    "\n",
    "if images.shape[1] != expected_channels:\n",
    "    print(f'Error: Expected {expected_channels} channels but got {images.shape[1]} channels')\n",
    "\n",
    "# Check if labels are in the expected format\n",
    "expected_label_channels = 1  # Adjust this if your labels have a different number of channels\n",
    "\n",
    "if labels.shape[1] != expected_label_channels:\n",
    "    print(f'Error: Expected {expected_label_channels} channels in labels but got {labels.shape[1]} channels')\n",
    "\n",
    "# Check the range of pixel values in images and labels\n",
    "print(f'Images min: {images.min()}, max: {images.max()}')\n",
    "print(f'Labels min: {labels.min()}, max: {labels.max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnTd2d-TmZv0"
   },
   "source": [
    "## Training and Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3574733,
     "status": "ok",
     "timestamp": 1727388479515,
     "user": {
      "displayName": "dazhuang wang",
      "userId": "16842954630206696630"
     },
     "user_tz": -60
    },
    "id": "BTNxuhhymmWj",
    "outputId": "938a1d86-9f98-4cc8-9249-2907792c8c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 120/120 [26:27<00:00, 13.23s/it, acc=0.746, f1=0.621, loss=0.447, precision=0.541, recall=0.774]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 completed. Train Loss: 0.5524, Train Acc: 0.7460, Train Precision: 0.5413, Train Recall: 0.7739, Train F1: 0.6206, Val Loss: 0.6744, Val Acc: 0.8078, Val Precision: 0.5946, Val Recall: 0.7853, Val F1: 0.6741, Duration: 1735.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:  50%|█████     | 60/120 [13:15<13:15, 13.25s/it, acc=0.838, f1=0.684, loss=0.431, precision=0.661, recall=0.727]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels out of bounds: min=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, max=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagate\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    105\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geopy/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/geopy/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming RSDataset and AttUNet classes are defined elsewhere and images_dir, labels_dir are provided\n",
    "# Use 3000 images per epoch\n",
    "max_samples = 3000\n",
    "dataset = RSDataset(images_dir, labels_dir, max_samples=max_samples)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Adjust batch size if GPU memory allows\n",
    "batch_size = 20  # Further reduce batch size\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Initialize the AttUNet model with 3 input channels and 1 output channel\n",
    "model = AttUNet(img_ch=3, output_ch=1)\n",
    "\n",
    "# Use binary cross-entropy as the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Set the Adam optimizer with an initial learning rate of 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Set the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Set the total number of training epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "def calculate_metrics(outputs, labels, threshold=0.5):\n",
    "    preds = (outputs > threshold).float().cpu().numpy().flatten()\n",
    "    labels = labels.float().cpu().numpy().flatten()\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_accuracy(outputs, labels, threshold=0.5):\n",
    "    preds = (outputs > threshold).float()\n",
    "    correct = (preds == labels).float().sum()\n",
    "    accuracy = correct / labels.numel()\n",
    "    return accuracy.item()\n",
    "\n",
    "def compute_iou(preds, labels, threshold=0.5):\n",
    "    preds = (preds > threshold).astype(float)  # Convert to float using NumPy's astype\n",
    "    intersection = (preds * labels).sum()\n",
    "    union = preds.sum() + labels.sum() - intersection\n",
    "    iou = intersection / union if union != 0 else 0.0\n",
    "    return iou\n",
    "\n",
    "# Gradient accumulation steps\n",
    "accumulation_steps = 4\n",
    "\n",
    "# Start training the model for a total of 50 epochs\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start time of the epoch\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "    progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "    for i, (images, labels) in progress_bar:\n",
    "        images = images.float().to(device, non_blocking=True)  # Convert images to float and move to device\n",
    "        labels = labels.float().to(device, non_blocking=True)  # Normalize labels to [0, 1] and move to device\n",
    "\n",
    "        # Ensure labels are within [0, 1]\n",
    "        labels = torch.clamp(labels, 0, 1)\n",
    "\n",
    "        outputs = model(images)  # Get model outputs\n",
    "        outputs = torch.sigmoid(outputs)  # Apply sigmoid to ensure outputs are in [0, 1]\n",
    "\n",
    "        # Adjust labels shape to match outputs shape if needed\n",
    "        if labels.ndim == 3:\n",
    "            labels = labels.unsqueeze(1)\n",
    "\n",
    "        # Check if labels are in the correct range\n",
    "        if labels.max() > 1 or labels.min() < 0:\n",
    "            raise ValueError(f\"Labels out of bounds: min={labels.min()}, max={labels.max()}\")\n",
    "\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "        loss.backward()  # Backpropagate\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()  # Update weights\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        epoch_loss += loss.item()  # Accumulate the loss\n",
    "        epoch_acc += calculate_accuracy(outputs, labels)  # Accumulate accuracy\n",
    "\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision, recall, f1 = calculate_metrics(outputs, labels)\n",
    "        epoch_precision += precision\n",
    "        epoch_recall += recall\n",
    "        epoch_f1 += f1\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item(), acc=epoch_acc / (i + 1), precision=epoch_precision / (i + 1), recall=epoch_recall / (i + 1), f1=epoch_f1 / (i + 1))  # Update progress bar with current loss and accuracy\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = epoch_loss / len(trainloader)\n",
    "    avg_train_acc = epoch_acc / len(trainloader)\n",
    "    avg_train_precision = epoch_precision / len(trainloader)\n",
    "    avg_train_recall = epoch_recall / len(trainloader)\n",
    "    avg_train_f1 = epoch_f1 / len(trainloader)\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_precision = 0\n",
    "    val_recall = 0\n",
    "    val_f1 = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valloader:\n",
    "            images = images.float().to(device, non_blocking=True)  # Convert images to float and move to device\n",
    "            labels = labels.float().to(device, non_blocking=True)  # Normalize labels to [0, 1] and move to device\n",
    "\n",
    "            # Ensure labels are within [0, 1]\n",
    "            labels = torch.clamp(labels, 0, 1)\n",
    "\n",
    "            outputs = model(images)  # Get model outputs\n",
    "            outputs = torch.sigmoid(outputs)  # Apply sigmoid to ensure outputs are in [0, 1]\n",
    "\n",
    "            # Adjust labels shape to match outputs shape if needed\n",
    "            if labels.ndim == 3:\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "            # Check if labels are in the correct range\n",
    "            if labels.max() > 1 or labels.min() < 0:\n",
    "                raise ValueError(f\"Labels out of bounds: min={labels.min()}, max={labels.max()}\")\n",
    "\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            val_loss += loss.item()  # Accumulate the loss\n",
    "            val_acc += calculate_accuracy(outputs, labels)  # Accumulate accuracy\n",
    "\n",
    "            # Calculate precision, recall, and F1 score\n",
    "            precision, recall, f1 = calculate_metrics(outputs, labels)\n",
    "            val_precision += precision\n",
    "            val_recall += recall\n",
    "            val_f1 += f1\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / len(valloader)\n",
    "    avg_val_acc = val_acc / len(valloader)\n",
    "    avg_val_precision = val_precision / len(valloader)\n",
    "    avg_val_recall = val_recall / len(valloader)\n",
    "    avg_val_f1 = val_f1 / len(valloader)\n",
    "\n",
    "    end_time = time.time()  # End time of the epoch\n",
    "    epoch_duration = end_time - start_time  # Duration of the epoch\n",
    "\n",
    "    tqdm.write(f'Epoch {epoch+1}/{num_epochs} completed. '\n",
    "               f'Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}, '\n",
    "               f'Train Precision: {avg_train_precision:.4f}, Train Recall: {avg_train_recall:.4f}, Train F1: {avg_train_f1:.4f}, '\n",
    "               f'Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}, '\n",
    "               f'Val Precision: {avg_val_precision:.4f}, Val Recall: {avg_val_recall:.4f}, Val F1: {avg_val_f1:.4f}, '\n",
    "               f'Duration: {epoch_duration:.2f} seconds')  # Print epoch summary\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    model_path = f'attention_unet_model/models_building_epoch_{epoch+1}.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save the final model after training is complete\n",
    "# torch.save(model.state_dict(), 'attention_unet_model/models_building_final.pth')\n",
    "\n",
    "# Evaluation on the validation set\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "all_iou = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in valloader:\n",
    "        images = images.float().to(device, non_blocking=True)\n",
    "        labels = labels.float().to(device, non_blocking=True)\n",
    "\n",
    "        # Ensure labels are within [0, 1]\n",
    "        labels = torch.clamp(labels, 0, 1)\n",
    "\n",
    "        outputs = model(images)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "\n",
    "        # Adjust labels shape to match outputs shape if needed\n",
    "        if labels.ndim == 3:\n",
    "            labels = labels.unsqueeze(1)\n",
    "\n",
    "        preds = (outputs > 0.5).float()\n",
    "\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            iou = compute_iou(all_preds[-1][i], all_labels[-1][i])\n",
    "            all_iou.append(iou)\n",
    "\n",
    "# Flatten lists to compute metrics\n",
    "all_labels = np.concatenate(all_labels).flatten()\n",
    "all_preds = np.concatenate(all_preds).flatten()\n",
    "\n",
    "# Compute precision, recall, and F1 score\n",
    "precision = precision_score(all_labels, all_preds)\n",
    "recall = recall_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "# Compute mean IoU\n",
    "mean_iou = np.mean(all_iou)\n",
    "\n",
    "print(f'Validation Mean IoU: {mean_iou:.4f}')\n",
    "print(f'Validation Precision: {precision:.4f}')\n",
    "print(f'Validation Recall: {recall:.4f}')\n",
    "print(f'Validation F1 Score: {f1:.4f}')\n",
    "\n",
    "# Markdown formatted summary\n",
    "markdown_summary = f\"\"\"\n",
    "# Model Evaluation Metrics\n",
    "\n",
    "**Precision:** {precision:.4f}\n",
    "\n",
    "**Recall:** {recall:.4f}\n",
    "\n",
    "**F1 Score:** {f1:.4f}\n",
    "\n",
    "**Mean IoU:** {mean_iou:.4f}\n",
    "\"\"\"\n",
    "\n",
    "print(markdown_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCwnB2Nmn4sy"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56819,
     "status": "ok",
     "timestamp": 1727420758733,
     "user": {
      "displayName": "dazhuang wang",
      "userId": "16842954630206696630"
     },
     "user_tz": -60
    },
    "id": "H8GNx5een6yy",
    "outputId": "69f21acd-4795-4cca-951b-5fe758c424d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37206/1681953995.py:208: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  batch_windows_tensor = torch.tensor(batch_windows).float().to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed large image and saved prediction to test_area_no_flip/attention_unet/test_area_predict.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from osgeo import gdal, gdal_array, osr\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Define ConvBlock and AttentionUNet models\n",
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(conv_block, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Up Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Block\n",
    "    \"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(Attention_block, self).__init__()\n",
    "\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        out = x * psi\n",
    "        return out\n",
    "\n",
    "class AttUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Unet implementation\n",
    "    Paper: https://arxiv.org/abs/1804.03999\n",
    "    \"\"\"\n",
    "    def __init__(self, img_ch=3, output_ch=1):\n",
    "        super(AttUNet, self).__init__()\n",
    "\n",
    "        n1 = 64\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(img_ch, filters[0])\n",
    "        self.Conv2 = conv_block(filters[0], filters[1])\n",
    "        self.Conv3 = conv_block(filters[1], filters[2])\n",
    "        self.Conv4 = conv_block(filters[2], filters[3])\n",
    "        self.Conv5 = conv_block(filters[3], filters[4])\n",
    "\n",
    "        self.Up5 = up_conv(filters[4], filters[3])\n",
    "        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n",
    "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
    "\n",
    "        self.Up4 = up_conv(filters[3], filters[2])\n",
    "        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n",
    "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
    "\n",
    "        self.Up3 = up_conv(filters[2], filters[1])\n",
    "        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n",
    "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
    "\n",
    "        self.Up2 = up_conv(filters[1], filters[0])\n",
    "        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n",
    "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
    "\n",
    "        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.Maxpool1(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.Maxpool2(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.Maxpool3(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.Maxpool4(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        d5 = self.Up5(e5)\n",
    "        x4 = self.Att5(g=d5, x=e4)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4, x=e3)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3, x=e2)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2, x=e1)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the pre-trained AttUNet model\n",
    "model = AttUNet(img_ch=3, output_ch=1)\n",
    "model.load_state_dict(torch.load('attention_unet_model/models_building_final.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Set input and output file paths\n",
    "input_image_path = 'test_area_no_flip/test_area.tif'\n",
    "output_image_path = 'test_area_no_flip/attention_unet/test_area_predict.tif'\n",
    "intermediate_folder = 'test_area_no_flip/attention_unet/intermediate'\n",
    "\n",
    "# Create intermediate results folder (if not exists)\n",
    "if not os.path.exists(intermediate_folder):\n",
    "    os.makedirs(intermediate_folder)\n",
    "\n",
    "# Open input image\n",
    "rsdataset = gdal.Open(input_image_path)\n",
    "\n",
    "# Read the first three bands and stack them into a numpy array\n",
    "images = np.stack([rsdataset.GetRasterBand(i).ReadAsArray() for i in range(1, 4)], axis=0)\n",
    "\n",
    "# Normalize to the range [0, 1]\n",
    "images = images / 255.0\n",
    "\n",
    "# Function: Overlap cropping\n",
    "def sliding_window(image, step_size, window_size):\n",
    "    for y in range(0, image.shape[1] - window_size[1] + 1, step_size[1]):\n",
    "        for x in range(0, image.shape[2] - window_size[0] + 1, step_size[0]):\n",
    "            yield (x, y, image[:, y:y + window_size[1], x:x + window_size[0]])\n",
    "\n",
    "# Initialize stitched image\n",
    "stitched_image = np.zeros((images.shape[1], images.shape[2]), dtype=np.uint8)\n",
    "\n",
    "# Overlap cropping and prediction\n",
    "window_size = (512, 512)\n",
    "step_size = (256, 256)\n",
    "batch_size = 8\n",
    "batch_windows = []\n",
    "batch_coords = []\n",
    "\n",
    "for (x, y, window) in sliding_window(images, step_size, window_size):\n",
    "    # Resize window to 256x256\n",
    "    resized_window = cv2.resize(window.transpose(1, 2, 0), (256, 256)).transpose(2, 0, 1)\n",
    "    batch_windows.append(resized_window)\n",
    "    batch_coords.append((x, y))\n",
    "\n",
    "    # If batch size is reached, perform prediction\n",
    "    if len(batch_windows) == batch_size:\n",
    "        batch_windows_tensor = torch.tensor(batch_windows).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_windows_tensor)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            output = (outputs[i] > 0.8).float().cpu().numpy()\n",
    "            prediction = cv2.resize(output.squeeze(), window_size)\n",
    "            prediction = (prediction > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "            # Update stitched image\n",
    "            x, y = batch_coords[i]\n",
    "            stitched_image[y:y + window_size[1], x:x + window_size[0]] = np.maximum(\n",
    "                stitched_image[y:y + window_size[1], x:x + window_size[0]], prediction\n",
    "            )\n",
    "\n",
    "        batch_windows = []\n",
    "        batch_coords = []\n",
    "\n",
    "# Process remaining windows\n",
    "if batch_windows:\n",
    "    batch_windows_tensor = torch.tensor(batch_windows).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_windows_tensor)\n",
    "\n",
    "    for i in range(len(batch_windows)):\n",
    "        output = (outputs[i] > 0.8).float().cpu().numpy()\n",
    "        prediction = cv2.resize(output.squeeze(), window_size)\n",
    "        prediction = (prediction > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "        # Update stitched image\n",
    "        x, y = batch_coords[i]\n",
    "        stitched_image[y:y + window_size[1], x:x + window_size[0]] = np.maximum(\n",
    "            stitched_image[y:y + window_size[1], x:x + window_size[0]], prediction\n",
    "        )\n",
    "\n",
    "# Create output image file\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "out_raster = driver.Create(output_image_path, rsdataset.RasterXSize, rsdataset.RasterYSize, 1, gdal.GDT_Byte)\n",
    "\n",
    "# Set georeferencing and projection information\n",
    "out_raster.SetGeoTransform(rsdataset.GetGeoTransform())\n",
    "out_raster.SetProjection(rsdataset.GetProjectionRef())\n",
    "\n",
    "# Write data to output image\n",
    "out_raster.GetRasterBand(1).WriteArray(stitched_image)\n",
    "\n",
    "# Close output image file\n",
    "out_raster.FlushCache()\n",
    "out_raster = None\n",
    "\n",
    "print(f\"Processed large image and saved prediction to {output_image_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "geopy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
